{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"Copy of Horse-or-Human-Small.ipynb","provenance":[{"file_id":"https://github.com/tinyMLx/colabs/blob/master/2-4-3-HorsesOrHumans.ipynb","timestamp":1606219208155}],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.9"}},"cells":[{"cell_type":"code","metadata":{"id":"zX4Kg8DUTKWO","executionInfo":{"status":"ok","timestamp":1606215294296,"user_tz":-660,"elapsed":947,"user":{"displayName":"","photoUrl":"","userId":""}}},"source":["#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n","# you may not use this file except in compliance with the License.\n","# You may obtain a copy of the License at\n","#\n","# https://www.apache.org/licenses/LICENSE-2.0\n","#\n","# Unless required by applicable law or agreed to in writing, software\n","# distributed under the License is distributed on an \"AS IS\" BASIS,\n","# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n","# See the License for the specific language governing permissions and\n","# limitations under the License."],"execution_count":8,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"M2TVqyYvVPIK"},"source":["## Download the neccessary data into the Colab Instance"]},{"cell_type":"code","metadata":{"id":"ioLbtB3uGKPX","executionInfo":{"status":"ok","timestamp":1606214245332,"user_tz":-660,"elapsed":1663,"user":{"displayName":"","photoUrl":"","userId":""}}},"source":["try:\n","  # %tensorflow_version only exists in Colab.\n","  %tensorflow_version 2.x\n","except Exception:\n","  pass"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RXZT2UsyIVe_","executionInfo":{"status":"ok","timestamp":1606219009059,"user_tz":-660,"elapsed":3729,"user":{"displayName":"","photoUrl":"","userId":""}},"outputId":"3925516e-0944-4afa-b0c2-8ef439a73422"},"source":["!wget --no-check-certificate \\\n","    https://storage.googleapis.com/laurencemoroney-blog.appspot.com/horse-or-human.zip \\\n","    -O /tmp/horse-or-human.zip\n","\n","!wget --no-check-certificate \\\n","    https://storage.googleapis.com/laurencemoroney-blog.appspot.com/horse-or-human.zip \\\n","    -O /tmp/validation-horse-or-human.zip"],"execution_count":1,"outputs":[{"output_type":"stream","text":["--2020-11-24 11:56:47--  https://storage.googleapis.com/laurencemoroney-blog.appspot.com/horse-or-human.zip\n","Resolving storage.googleapis.com (storage.googleapis.com)... 172.217.9.208, 172.217.2.112, 172.217.164.144, ...\n","Connecting to storage.googleapis.com (storage.googleapis.com)|172.217.9.208|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 149574867 (143M) [application/zip]\n","Saving to: ‘/tmp/horse-or-human.zip’\n","\n","/tmp/horse-or-human 100%[===================>] 142.65M   174MB/s    in 0.8s    \n","\n","2020-11-24 11:56:48 (174 MB/s) - ‘/tmp/horse-or-human.zip’ saved [149574867/149574867]\n","\n","--2020-11-24 11:56:48--  https://storage.googleapis.com/laurencemoroney-blog.appspot.com/horse-or-human.zip\n","Resolving storage.googleapis.com (storage.googleapis.com)... 172.217.12.240, 172.253.63.128, 142.250.31.128, ...\n","Connecting to storage.googleapis.com (storage.googleapis.com)|172.217.12.240|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 149574867 (143M) [application/zip]\n","Saving to: ‘/tmp/validation-horse-or-human.zip’\n","\n","/tmp/validation-hor 100%[===================>] 142.65M   201MB/s    in 0.7s    \n","\n","2020-11-24 11:56:49 (201 MB/s) - ‘/tmp/validation-horse-or-human.zip’ saved [149574867/149574867]\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"PLy3pthUS0D2","executionInfo":{"status":"ok","timestamp":1606219144654,"user_tz":-660,"elapsed":2412,"user":{"displayName":"","photoUrl":"","userId":""}}},"source":["import os\n","import zipfile\n","\n","local_zip = '/tmp/horse-or-human.zip'\n","zip_ref = zipfile.ZipFile(local_zip, 'r')\n","zip_ref.extractall('/tmp/horse-or-human')\n","local_zip = '/tmp/validation-horse-or-human.zip'\n","zip_ref = zipfile.ZipFile(local_zip, 'r')\n","zip_ref.extractall('/tmp/validation-horse-or-human')\n","zip_ref.close()"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"u6eO4B8kVPIK","executionInfo":{"status":"ok","timestamp":1606219148103,"user_tz":-660,"elapsed":1548,"user":{"displayName":"","photoUrl":"","userId":""}},"outputId":"83722b2e-0fcc-4244-d767-ce540e47487a","colab":{"base_uri":"https://localhost:8080/"}},"source":["# Directory with our training horse pictures\n","train_horse_dir = os.path.join('/tmp/horse-or-human/horses')\n","# Directory with our training human pictures\n","train_human_dir = os.path.join('/tmp/horse-or-human/humans')\n","# Directory with our training horse pictures\n","validation_horse_dir = os.path.join('/tmp/validation-horse-or-human/horses')\n","# Directory with our training human pictures\n","validation_human_dir = os.path.join('/tmp/validation-horse-or-human/humans')\n","train_horse_names = os.listdir('/tmp/horse-or-human/horses')\n","print(train_horse_names[:10])\n","train_human_names = os.listdir('/tmp/horse-or-human/humans')\n","print(train_human_names[:10])\n","validation_horse_hames = os.listdir('/tmp/validation-horse-or-human/horses')\n","print(validation_horse_hames[:10])\n","validation_human_names = os.listdir('/tmp/validation-horse-or-human/humans')\n","print(validation_human_names[:10])"],"execution_count":4,"outputs":[{"output_type":"stream","text":["['horse15-8.png', 'horse32-7.png', 'horse07-9.png', 'horse11-5.png', 'horse50-4.png', 'horse41-1.png', 'horse29-1.png', 'horse41-5.png', 'horse24-9.png', 'horse48-5.png']\n","['human10-17.png', 'human14-14.png', 'human10-06.png', 'human07-01.png', 'human11-01.png', 'human04-15.png', 'human08-11.png', 'human12-16.png', 'human16-25.png', 'human16-23.png']\n","['horse15-8.png', 'horse32-7.png', 'horse07-9.png', 'horse11-5.png', 'horse50-4.png', 'horse41-1.png', 'horse29-1.png', 'horse41-5.png', 'horse24-9.png', 'horse48-5.png']\n","['human10-17.png', 'human14-14.png', 'human10-06.png', 'human07-01.png', 'human11-01.png', 'human04-15.png', 'human08-11.png', 'human12-16.png', 'human16-25.png', 'human16-23.png']\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"qvfZg3LQbD-5","executionInfo":{"status":"ok","timestamp":1606219172502,"user_tz":-660,"elapsed":2093,"user":{"displayName":"","photoUrl":"","userId":""}}},"source":["import tensorflow as tf"],"execution_count":5,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Lvw9G26cVPIK"},"source":["## Define your model and optimizer"]},{"cell_type":"code","metadata":{"id":"PixZ2s5QbYQ3","executionInfo":{"status":"ok","timestamp":1606219201028,"user_tz":-660,"elapsed":6393,"user":{"displayName":"","photoUrl":"","userId":""}}},"source":["model = tf.keras.models.Sequential([\n","    # Note the input shape is the desired size of the image with 3 bytes color\n","    # This is the first convolution\n","    tf.keras.layers.Conv2D(16, (3,3), activation='relu', input_shape=(300, 300, 3)),\n","    tf.keras.layers.MaxPooling2D(2, 2),\n","    # The second convolution\n","    tf.keras.layers.Conv2D(32, (3,3), activation='relu'),\n","    tf.keras.layers.MaxPooling2D(2,2),\n","    # The third convolution\n","    tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n","    tf.keras.layers.MaxPooling2D(2,2),\n","    # The fourth convolution\n","    tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n","    tf.keras.layers.MaxPooling2D(2,2),\n","    # Flatten the results to feed into a DNN\n","    tf.keras.layers.Flatten(),\n","    # 512 neuron hidden layer\n","    tf.keras.layers.Dense(512, activation='relu'),\n","    tf.keras.layers.Dense(128, activation='relu'),\n","    # Only 1 output neuron. It will contain a value from 0-1 where 0 for 1 class ('horses') and 1 for the other ('humans')\n","    tf.keras.layers.Dense(1, activation='sigmoid')\n","])"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"pMDg5mJxDrBq"},"source":["print(model.summary())"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8DHWhFP_uhq3"},"source":["from tensorflow.keras.optimizers import RMSprop\n","optimizer = RMSprop(lr=0.001)\n","model.compile(loss='binary_crossentropy',\n","              optimizer=optimizer,\n","              metrics=['acc'])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kxArJhzYVPIK"},"source":["## Organize your data into Generators"]},{"cell_type":"code","metadata":{"id":"ClebU9NJg99G"},"source":["from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","\n","# All images will be augmented\n","train_datagen = ImageDataGenerator(\n","      rescale=1./255,\n","      #rotation_range=40,\n","      #width_shift_range=0.2,\n","      #height_shift_range=0.2,\n","      #shear_range=0.2,\n","      #zoom_range=0.2,\n","      #horizontal_flip=True,\n","      #fill_mode='nearest'\n","      )\n","\n","# Flow training images in batches of 128 using train_datagen generator\n","train_generator = train_datagen.flow_from_directory(\n","        '/tmp/horse-or-human/',  # This is the source directory for training images\n","        target_size=(300, 300),  # All images will be resized to 300x300\n","        batch_size=128,\n","        # Since we use binary_crossentropy loss, we need binary labels\n","        class_mode='binary')\n","\n","validation_datagen = ImageDataGenerator(rescale=1/255)\n","\n","validation_generator = validation_datagen.flow_from_directory(\n","        '/tmp/validation-horse-or-human',\n","        target_size=(300, 300),\n","        class_mode='binary')\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VTgf41wTVPIL"},"source":["## Train your model\n","This may take a little while. Remember we are now building and training relatively complex computer vision models!"]},{"cell_type":"code","metadata":{"id":"Fb1_lgobv81m"},"source":["history = model.fit(\n","      train_generator,\n","      steps_per_epoch=8,  \n","      epochs=15,\n","      verbose=1,\n","      validation_data=validation_generator)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"o6vSHzPR2ghH"},"source":["## Run your Model\n","\n","Let's now take a look at actually running a prediction using the model. This code will allow you to choose 1 or more files from your file system, it will then upload them, and run them through the model, giving an indication of whether the object is a horse or a human. **Was the model correct? Try a couple more images and see if you can confuse it!**"]},{"cell_type":"code","metadata":{"id":"DoWp43WxJDNT"},"source":["import numpy as np\n","from google.colab import files\n","from keras.preprocessing import image\n","\n","uploaded = files.upload()\n","\n","for fn in uploaded.keys():\n"," \n","  # predicting images\n","  path = '/content/' + fn\n","  img = image.load_img(path, target_size=(300, 300))\n","  x = image.img_to_array(img)\n","  x = np.expand_dims(x, axis=0)\n","\n","  image_tensor = np.vstack([x])\n","  classes = model.predict(image_tensor)\n","  print(classes)\n","  print(classes[0])\n","  if classes[0]>0.5:\n","    print(fn + \" is a human\")\n","  else:\n","    print(fn + \" is a horse\")\n"," "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"m9AoLglDVPIL"},"source":["## Finally lets visualize all of the model layers!"]},{"cell_type":"code","metadata":{"id":"h-SUwB0bzVvc"},"source":["import matplotlib.pyplot as plt\n","import numpy as np\n","import random\n","from tensorflow.keras.preprocessing.image import img_to_array, load_img\n","\n","# Let's define a new Model that will take an image as input, and will output\n","# intermediate representations for all layers in the previous model after\n","# the first.\n","successive_outputs = [layer.output for layer in model.layers[1:]]\n","#visualization_model = Model(img_input, successive_outputs)\n","visualization_model = tf.keras.models.Model(inputs = model.input, outputs = successive_outputs)\n","# Let's prepare a random input image from the training set.\n","horse_img_files = [os.path.join(train_horse_dir, f) for f in train_horse_names]\n","human_img_files = [os.path.join(train_human_dir, f) for f in train_human_names]\n","img_path = random.choice(horse_img_files + human_img_files)\n","\n","img = load_img(img_path, target_size=(300, 300))  # this is a PIL image\n","x = img_to_array(img)  # Numpy array with shape (300, 300, 3)\n","x = x.reshape((1,) + x.shape)  # Numpy array with shape (1, 300, 300, 3)\n","\n","# Rescale by 1/255\n","x /= 255\n","\n","# Let's run our image through our network, thus obtaining all\n","# intermediate representations for this image.\n","successive_feature_maps = visualization_model.predict(x)\n","\n","# These are the names of the layers, so can have them as part of our plot\n","layer_names = [layer.name for layer in model.layers]\n","\n","# Now let's display our representations\n","for layer_name, feature_map in zip(layer_names, successive_feature_maps):\n","  if len(feature_map.shape) == 4:\n","    # Just do this for the conv / maxpool layers, not the fully-connected layers\n","    n_features = feature_map.shape[-1]  # number of features in feature map\n","    # The feature map has shape (1, size, size, n_features)\n","    size = feature_map.shape[1]\n","    # We will tile our images in this matrix\n","    display_grid = np.zeros((size, size * n_features))\n","    for i in range(n_features):\n","      # Postprocess the feature to make it visually palatable\n","      x = feature_map[0, :, :, i]\n","      x -= x.mean()\n","      x /= x.std()\n","      x *= 64\n","      x += 128\n","      x = np.clip(x, 0, 255).astype('uint8')\n","      # We'll tile each filter into this big horizontal grid\n","      display_grid[:, i * size : (i + 1) * size] = x\n","    # Display the grid\n","    scale = 20. / n_features\n","    plt.figure(figsize=(scale * n_features, scale))\n","    plt.title(layer_name)\n","    plt.grid(False)\n","    plt.imshow(display_grid, aspect='auto', cmap='viridis')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"j4IBgYCYooGD"},"source":["## Clean Up\n","\n","Before running the next exercise, run the following cell to terminate the kernel and free memory resources:"]},{"cell_type":"code","metadata":{"id":"651IgjLyo-Jx"},"source":["import os, signal\n","os.kill(os.getpid(), signal.SIGKILL)"],"execution_count":null,"outputs":[]}]}